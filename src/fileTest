{
  "paragraphs": [
    {
      "text": "%sh\necho this is a new file",
      "dateUpdated": "Jun 27, 2016 1:30:03 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/sh"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1467033356137_2137786238",
      "id": "20160627-131556_521529768",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "this is a new file\n"
      },
      "dateCreated": "Jun 27, 2016 1:15:56 PM",
      "dateStarted": "Jun 27, 2016 1:30:03 PM",
      "dateFinished": "Jun 27, 2016 1:30:04 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sh\necho this is jst another para",
      "dateUpdated": "Jun 27, 2016 5:48:31 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/sh"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1467034203971_100736738",
      "id": "20160627-133003_796626784",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "this is jst another para\n"
      },
      "dateCreated": "Jun 27, 2016 1:30:03 PM",
      "dateStarted": "Jun 27, 2016 5:48:31 PM",
      "dateFinished": "Jun 27, 2016 5:48:34 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sh \nls\n",
      "dateUpdated": "Jun 27, 2016 5:56:10 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/sh"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1467049711118_1863808769",
      "id": "20160627-174831_331684261",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "bin\nconf\nexample-notebook\nhustler\ninterpreter\nlogs\nnotebook\nR\nrun\nwebapps\nzeppelin-server\nzeppelin-web\nzeppelin-zengine\n"
      },
      "dateCreated": "Jun 27, 2016 5:48:31 PM",
      "dateStarted": "Jun 27, 2016 5:56:10 PM",
      "dateFinished": "Jun 27, 2016 5:56:10 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sql\nselect * from default_qubole_memtracker limit 1",
      "dateUpdated": "Jun 27, 2016 5:50:25 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/sql"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1467049721483_-449686368",
      "id": "20160627-174841_1803092211",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "org.apache.thrift.transport.TTransportException\n\tat org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)\n\tat org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)\n\tat org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429)\n\tat org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318)\n\tat org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219)\n\tat org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:69)\n\tat org.apache.zeppelin.interpreter.thrift.RemoteInterpreterService$Client.recv_getFormType(RemoteInterpreterService.java:311)\n\tat org.apache.zeppelin.interpreter.thrift.RemoteInterpreterService$Client.getFormType(RemoteInterpreterService.java:298)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreter.getFormType(RemoteInterpreter.java:306)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.getFormType(LazyOpenInterpreter.java:122)\n\tat org.apache.zeppelin.notebook.Paragraph.jobRun(Paragraph.java:211)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:174)\n\tat org.apache.zeppelin.scheduler.RemoteScheduler$JobRunner.run(RemoteScheduler.java:348)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n"
      },
      "dateCreated": "Jun 27, 2016 5:48:41 PM",
      "dateStarted": "Jun 27, 2016 5:50:25 PM",
      "dateFinished": "Jun 27, 2016 5:50:25 PM",
      "status": "ERROR",
      "errorMessage": "org.apache.thrift.transport.TTransportException\n\tat org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)\n\tat org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)\n\tat org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429)\n\tat org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318)\n\tat org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219)\n\tat org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:69)\n\tat org.apache.zeppelin.interpreter.thrift.RemoteInterpreterService$Client.recv_getFormType(RemoteInterpreterService.java:311)\n\tat org.apache.zeppelin.interpreter.thrift.RemoteInterpreterService$Client.getFormType(RemoteInterpreterService.java:298)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreter.getFormType(RemoteInterpreter.java:306)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.getFormType(LazyOpenInterpreter.java:122)\n\tat org.apache.zeppelin.notebook.Paragraph.jobRun(Paragraph.java:211)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:174)\n\tat org.apache.zeppelin.scheduler.RemoteScheduler$JobRunner.run(RemoteScheduler.java:348)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "desc tables;",
      "dateUpdated": "Jun 28, 2016 5:21:09 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1467049734897_1414067953",
      "id": "20160627-174854_1408795197",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "\u003cconsole\u003e:28: error: missing arguments for method desc in object functions;\nfollow this method with `_\u0027 if you want to treat it as a partially applied function\n              desc tables;\n              ^\n"
      },
      "dateCreated": "Jun 27, 2016 5:48:54 PM",
      "dateStarted": "Jun 28, 2016 5:21:10 PM",
      "dateFinished": "Jun 28, 2016 5:21:10 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sql\nselect * from  default_qubole_airline_origin_destination limit 100 where itind like \u0027monika\u0027",
      "dateUpdated": "Jun 28, 2016 5:23:13 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "itinid",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "mktid",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "itinid",
              "index": 0.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "mktid",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        },
        "enabled": true,
        "editorMode": "ace/mode/sql"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1467134469974_-801565499",
      "id": "20160628-172109_1509220172",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "org.apache.spark.sql.AnalysisException: missing EOF at \u0027where\u0027 near \u0027100\u0027; line 1 pos 67\n\tat org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:318)\n\tat org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)\n\tat org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)\n\tat scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)\n\tat scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)\n\tat scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)\n\tat scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)\n\tat scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)\n\tat scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)\n\tat scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)\n\tat scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)\n\tat scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)\n\tat scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)\n\tat scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)\n\tat scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)\n\tat scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)\n\tat scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)\n\tat scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)\n\tat org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)\n\tat org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:295)\n\tat org.apache.spark.sql.hive.HiveQLDialect$$anonfun$parse$1.apply(HiveContext.scala:66)\n\tat org.apache.spark.sql.hive.HiveQLDialect$$anonfun$parse$1.apply(HiveContext.scala:66)\n\tat org.apache.spark.sql.hive.client.ClientWrapper$$anonfun$withHiveState$1.apply(ClientWrapper.scala:291)\n\tat org.apache.spark.sql.hive.client.ClientWrapper.liftedTree1$1(ClientWrapper.scala:238)\n\tat org.apache.spark.sql.hive.client.ClientWrapper.retryLocked(ClientWrapper.scala:237)\n\tat org.apache.spark.sql.hive.client.ClientWrapper.withHiveState(ClientWrapper.scala:280)\n\tat org.apache.spark.sql.hive.HiveQLDialect.parse(HiveContext.scala:65)\n\tat org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:211)\n\tat org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:211)\n\tat org.apache.spark.sql.execution.SparkSQLParser$$anonfun$org$apache$spark$sql$execution$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)\n\tat org.apache.spark.sql.execution.SparkSQLParser$$anonfun$org$apache$spark$sql$execution$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:113)\n\tat scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)\n\tat scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)\n\tat scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)\n\tat scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)\n\tat scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)\n\tat scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)\n\tat scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)\n\tat scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)\n\tat scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)\n\tat scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)\n\tat scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)\n\tat scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)\n\tat scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)\n\tat scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)\n\tat scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)\n\tat org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)\n\tat org.apache.spark.sql.SQLContext$$anonfun$1.apply(SQLContext.scala:208)\n\tat org.apache.spark.sql.SQLContext$$anonfun$1.apply(SQLContext.scala:208)\n\tat org.apache.spark.sql.execution.datasources.DDLParser.parse(DDLParser.scala:43)\n\tat org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:231)\n\tat org.apache.spark.sql.hive.HiveContext.parseSql(HiveContext.scala:331)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:817)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.zeppelin.spark.SparkSqlInterpreter.interpret(SparkSqlInterpreter.java:158)\n\tat org.apache.zeppelin.interpreter.ClassloaderInterpreter.interpret(ClassloaderInterpreter.java:57)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:104)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:316)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:178)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:160)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n"
      },
      "dateCreated": "Jun 28, 2016 5:21:09 PM",
      "dateStarted": "Jun 28, 2016 5:23:13 PM",
      "dateFinished": "Jun 28, 2016 5:23:13 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1467134524660_-999883038",
      "id": "20160628-172204_19949352",
      "dateCreated": "Jun 28, 2016 5:22:04 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Hello123",
  "id": "4UYQRFG9B71467033355",
  "angularObjects": {
    "2BPXF6G95112501467131297169": [],
    "2BNF6CD2H112501467131297166": [],
    "2BNTV37W5112501467131297134": []
  },
  "config": {},
  "info": {},
  "source": "FCN"
}